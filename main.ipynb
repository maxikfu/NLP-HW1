{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\norig_stdout = sys.stdout\\nfout = open('wrongpredictions.txt', 'w')\\nsys.stdout = fout\\nprint('Accuracy ',cmpFiles('submission.txt','dataset/dev.conll'))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFilePath = 'dataset/train.conll'\n",
    "testFilePath = 'dataset/test.conll'\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def counting_tri_bi_uni_grams(training_file_path):\n",
    "    sentence_counter = 0\n",
    "    previous_tag = None\n",
    "    tag_count_dictionary = {}\n",
    "    tag_count_dictionary['START'] = 0\n",
    "    tag_count_dictionary['END'] = 0\n",
    "    spa_tag_count={}\n",
    "    eng_tag_count= {}\n",
    "    transition_count = {}\n",
    "    emission_count = {}\n",
    "    word_counting = {}\n",
    "    token_language_count = {}\n",
    "    spa_emission_count = {}\n",
    "    eng_emission_count = {}\n",
    "    tag1 = None\n",
    "    tag2 = None\n",
    "    tag3 = None\n",
    "    for line in open(training_file_path, 'r', encoding='UTF-8'):\n",
    "        list_line = []\n",
    "        for col in line.split():\n",
    "            list_line.append(col)\n",
    "        ############################computing bigrams and unigrams for tags#######################################\n",
    "        if len(list_line) == 0:  # sentence ended computing transition from tag to end state\n",
    "            if tuple([previous_tag,'END']) in transition_count:\n",
    "                transition_count[tuple([previous_tag,'END'])] += 1\n",
    "            else:\n",
    "                transition_count[tuple([previous_tag,'END'])] = 1\n",
    "            tag3 = 'END'\n",
    "            if tag1 is not None and tag2 is not None and tag3 is not None:\n",
    "                if tuple([tag1, tag2, tag3]) in transition_count:\n",
    "                    transition_count[tuple([tag1, tag2, tag3])] += 1\n",
    "                else:\n",
    "                    transition_count[tuple([tag1, tag2, tag3])] = 1\n",
    "            tag1 = None\n",
    "            tag2 = None\n",
    "            tag3 = None\n",
    "            previous_tag = None  # for computing start to tag\n",
    "        else:\n",
    "            # for smoothing\n",
    "\n",
    "            if list_line[0] in word_counting:\n",
    "                word_counting[list_line[0]] += 1\n",
    "            else:\n",
    "                word_counting[list_line[0]] = 1\n",
    "            tu = tuple([list_line[0], list_line[1]])\n",
    "            if tu in token_language_count:\n",
    "                token_language_count[tu] += 1\n",
    "            else:\n",
    "                token_language_count[tu] = 1\n",
    "            if list_line[2] in tag_count_dictionary:  # counting tag occurrences\n",
    "                tag_count_dictionary[list_line[2]] += 1  # counting delimiter for emission\n",
    "            else:\n",
    "                tag_count_dictionary[list_line[2]] = 1\n",
    "#spa and eng tag count separetly\n",
    "            if list_line[1] in ['spa']:\n",
    "                if list_line[2] in spa_tag_count:\n",
    "                    spa_tag_count[list_line[2]] += 1\n",
    "                else:\n",
    "                    spa_tag_count[list_line[2]] = 1\n",
    "            if list_line[1] in ['eng']:\n",
    "                if list_line[2] in eng_tag_count:\n",
    "                    eng_tag_count[list_line[2]] += 1\n",
    "                else:\n",
    "                    eng_tag_count[list_line[2]] = 1\n",
    "\n",
    "            if previous_tag is None:  # computing probability from START to tag\n",
    "                tag1 = 'START'\n",
    "                tag2 = list_line[2]\n",
    "                sentence_counter += 1\n",
    "                tag_count_dictionary['START'] += 1\n",
    "                tag_count_dictionary['END'] += 1\n",
    "                if tuple(['START', list_line[2]]) in transition_count:  # increasing counter\n",
    "                    transition_count[tuple(['START', list_line[2]])] += 1\n",
    "                else:  # creating new one\n",
    "                    transition_count[tuple(['START', list_line[2]])] = 1\n",
    "            else:  # we are in the middle of sentence\n",
    "                tag3 = list_line[2]\n",
    "                if tuple([previous_tag,list_line[2]]) in transition_count:\n",
    "                    transition_count[tuple([previous_tag,list_line[2]])] += 1\n",
    "                else:\n",
    "                    transition_count[tuple([previous_tag,list_line[2]])] = 1\n",
    "                if tag1 is not None and tag2 is not None and tag3 is not None:\n",
    "                    if tuple([tag1, tag2, tag3]) in transition_count:\n",
    "                        transition_count[tuple([tag1, tag2, tag3])] += 1\n",
    "                    else:\n",
    "                        transition_count[tuple([tag1, tag2, tag3])] = 1\n",
    "                    tag1 = tag2\n",
    "                    tag2 = tag3\n",
    "                    tag3 = None\n",
    "\n",
    "            previous_tag = list_line[2]\n",
    "            if tuple([list_line[2], list_line[0]]) in emission_count:  # counting emission probability\n",
    "                emission_count[tuple([list_line[2], list_line[0]])] += 1\n",
    "            else:\n",
    "                emission_count[tuple([list_line[2], list_line[0]])] = 1\n",
    "#if we meet word what starts with upper case it amy be a name, then probability its PROPN bigger\n",
    "            if list_line[0][0].isupper() and len(list_line[0]) > 1:\n",
    "                if tuple([list_line[2], 'name']) in emission_count:  # counting emission probability\n",
    "                    emission_count[tuple([list_line[2], 'name'])] += 1\n",
    "                else:\n",
    "                    emission_count[tuple([list_line[2], 'name'])] = 1\n",
    "#counting for spa and eng words separate\n",
    "            if list_line[1] in ['spa'] and tuple([list_line[2], list_line[0]]) in spa_emission_count:\n",
    "                spa_emission_count[tuple([list_line[2], list_line[0]])] += 1\n",
    "            elif list_line[1] in ['spa']:\n",
    "                spa_emission_count[tuple([list_line[2], list_line[0]])] = 1\n",
    "\n",
    "            if list_line[1] in ['eng'] and tuple([list_line[2], list_line[0]]) in eng_emission_count:\n",
    "                eng_emission_count[tuple([list_line[2], list_line[0]])] += 1\n",
    "            elif list_line[1] in ['eng']:\n",
    "                eng_emission_count[tuple([list_line[2], list_line[0]])] = 1\n",
    "    if tuple(['END', previous_tag]) in transition_count:\n",
    "        transition_count[tuple([previous_tag, 'END'])] += 1\n",
    "    else:\n",
    "        transition_count[tuple([previous_tag,'END'])] = 1\n",
    "    tag3 = 'END'\n",
    "    if tag1 is not None and tag2 is not None and tag3 is not None:\n",
    "        if tuple([tag1, tag2, tag3]) in transition_count:\n",
    "            transition_count[tuple([tag1, tag2, tag3])] += 1\n",
    "        else:\n",
    "            transition_count[tuple([tag1, tag2, tag3])] = 1\n",
    "    # COUNtING PROBABILITY FOR UNSEEN WORDS WITH WORDS OCCURRED ONLY ONCE\n",
    "    unseen_dictionary = {}\n",
    "    for word, value in word_counting.items():\n",
    "        if value == 1:  # this word occurs only once so we need to add (tag,unseen_word) into dictionary\n",
    "            for key, v in emission_count.items():\n",
    "                if word in key:\n",
    "                    if tuple([key[0], 'unseen_word']) in unseen_dictionary:\n",
    "                        unseen_dictionary[tuple([key[0], 'unseen_word'])] += 1\n",
    "                    else:\n",
    "                        unseen_dictionary[tuple([key[0], 'unseen_word'])] = 1\n",
    "    emission_count.update(unseen_dictionary)\n",
    "    return emission_count,spa_emission_count,eng_emission_count,tag_count_dictionary,spa_tag_count,\\\n",
    "           eng_tag_count,transition_count, word_counting, sentence_counter,token_language_count\n",
    "\n",
    "\n",
    "def deleted_interpolation(transition_matrix,tag_count ,token_dictionary):\n",
    "    lambda_vector = [0, 0, 0]\n",
    "    list_variable =[]\n",
    "    n = 0\n",
    "    for k, v in token_dictionary.items():\n",
    "        n += v\n",
    "    for key,value in transition_matrix.items():\n",
    "        if len(key) == 3 and value > 0:\n",
    "            if transition_matrix[tuple([key[0],key[1]])]>1:\n",
    "                list_variable.append((transition_matrix[key]-1)/(transition_matrix[tuple([key[0],key[1]])]-1))\n",
    "            else:\n",
    "                list_variable.append(0)\n",
    "            if tag_count[key[0]] > 1:\n",
    "                list_variable.append((transition_matrix[tuple([key[1],key[2]])]-1)/(tag_count[key[1]]-1))\n",
    "            else:\n",
    "                list_variable.append(0)\n",
    "\n",
    "            list_variable.append((tag_count[key[2]]-1)/(n-1))\n",
    "            max_index = np.argmax(list_variable)\n",
    "            list_variable = []\n",
    "            if max_index == 0:\n",
    "                lambda_vector[0] += value\n",
    "            elif max_index == 1:\n",
    "                lambda_vector[1] += value\n",
    "            else:\n",
    "                lambda_vector[2] += value\n",
    "    #normalization step\n",
    "    norm_lambda = [float(i) / sum(lambda_vector) for i in lambda_vector]\n",
    "    return norm_lambda\n",
    "\n",
    "\n",
    "def choosing_language(lang,emis,spa_emis,eng_emis,tag,spa_tag,eng_tag):\n",
    "    lan_emission = None\n",
    "    lan_tag = None\n",
    "    if lang == 'eng':\n",
    "        lan_emission = eng_emis\n",
    "        lan_tag = eng_tag\n",
    "    elif lang == 'spa':\n",
    "        lan_emission = spa_emis\n",
    "        lan_tag = spa_tag\n",
    "    else:\n",
    "        lan_emission = emis\n",
    "        lan_tag = tag\n",
    "    return lan_emission\n",
    "\n",
    "\n",
    "def viterbi(test_data_path, state_graph, tag_count, transition_count, emission_count, word_counting,norm_lambda,\\\n",
    "            spa_emiss,eng_emiss,spa_tag,eng_tag,token_language):\n",
    "    sentence_counter = tag_count['START']\n",
    "    sentences = []  # aka OBSERVATIONS\n",
    "    language_in_sentence = []  # just easier to print that in the file\n",
    "    language = []\n",
    "    sentence = []\n",
    "    n = 0\n",
    "    for k, v in word_counting.items():\n",
    "        n += v\n",
    "    for line in open(test_data_path, 'r', encoding='UTF-8'):\n",
    "        if len(line.split()) == 0:  # end of sentence so we can start our calculation for this sentence\n",
    "            sentences.append(sentence)\n",
    "            language_in_sentence.append(language)\n",
    "            sentence = []\n",
    "            language = []\n",
    "        else:\n",
    "            sentence.append(line.split()[0])\n",
    "            language.append(line.split()[1])\n",
    "    # taking care of the last sentence\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "        language_in_sentence.append(language)\n",
    "\n",
    "    orig_stdout = sys.stdout\n",
    "    fout = open('submission.txt', 'w')\n",
    "    sys.stdout = fout\n",
    "\n",
    "    for sentence, lang in zip(sentences, language_in_sentence):\n",
    "        k = len(sentence)  # number of time steps\n",
    "        viterbi_matrix = {}\n",
    "        backpointer = {}\n",
    "#choosing dictionary based on language\n",
    "        emission = choosing_language(lang[0],emission_count,spa_emiss,eng_emiss,tag_count,spa_tag,eng_tag)\n",
    "        #emission = emission_count  # all spa and eng words together\n",
    "        tag_variable = tag_count\n",
    "        transition_variable = transition_count\n",
    "        word_counting_variable = word_counting\n",
    "        for state in state_graph:  # initialization step\n",
    "            state_value = []\n",
    "            if tuple(['START',state]) in transition_variable:\n",
    "                a = transition_variable[tuple(['START',state])] / sentence_counter\n",
    "            else:\n",
    "                a = 0\n",
    "            if tuple([state, sentence[0]]) in emission:  # this word in dictionary\n",
    "                c = emission[tuple([state, sentence[0]])]\n",
    "                b = c / tag_variable[state]\n",
    "            elif tuple([sentence[0],lang[0]]) in token_language:\n",
    "#to avoid 0 when word in training set marked as different language but exists in general\n",
    "                b = 0\n",
    "            else:#unseen words\n",
    "                if sentence[0][0].isupper():  # if First letter of the word Upper case more likely it will be PROPN\n",
    "                    if tuple([state,'name']) in emission_count:\n",
    "                        b = emission_count[tuple([state, 'name'])] / tag_variable[state]\n",
    "                    else:\n",
    "                        b = 0\n",
    "                else:\n",
    "                    if tuple([state, 'unseen_word']) in emission_count:\n",
    "                        b = emission_count[tuple([state, 'unseen_word'])] / tag_variable[state]\n",
    "                    else:\n",
    "                        if state in tag_variable:\n",
    "                            b = 1 / tag_variable[state]\n",
    "                        else:\n",
    "                            b = 0\n",
    "            state_value.append(a * b)\n",
    "            viterbi_matrix[state] = state_value  # initializing viterbi matrix\n",
    "            backpointer[state] = ['START']\n",
    "        # recursion step\n",
    "        state_graph_extended = ['START'] + state_graph\n",
    "        for t in range(1, k):\n",
    "            aaa = 0\n",
    "#choosing dictionary based on language\n",
    "            emission = choosing_language(lang[t], emission_count, spa_emiss, eng_emiss, tag_count,\\\n",
    "                                                       spa_tag, eng_tag)\n",
    "            for state in state_graph:\n",
    "                links_value = []  # from this values we are going to choose maximum value\n",
    "                backtrack_values = []\n",
    "                for previous_step_state in state_graph:  # computing new value based on previouse step and choosing\n",
    "                    # maximum at the end\n",
    "                    if tuple([previous_step_state,state]) in transition_variable:  # not new transition between tags\n",
    "                        a = transition_variable[tuple([previous_step_state,state])] / tag_variable[\n",
    "                            previous_step_state]\n",
    "                    else:\n",
    "                        a = 0\n",
    "                    if tuple([state, sentence[t]]) in emission:  # not new word to this tag\n",
    "                        c = emission[tuple([state, sentence[t]])]\n",
    "                        b = c / tag_variable[state]\n",
    "                    elif tuple([sentence[t], lang[t]]) in token_language:\n",
    "                        # avoiding if languages got messed up\n",
    "                        b = 0\n",
    "                    else:  # unseen words handels here\n",
    "                        if sentence[t][0].isupper():\n",
    "                            if tuple([state,'name']) in emission_count:\n",
    "                                b = emission_count[tuple([state,'name'])] / tag_variable[state]\n",
    "                            else:\n",
    "                                b=0\n",
    "                        else:\n",
    "                            if tuple([state, 'unseen_word']) in emission_count:\n",
    "                                b = emission_count[tuple([state, 'unseen_word'])] / tag_variable[state]\n",
    "                            else:\n",
    "                                if state in tag_variable:\n",
    "                                    b = 1 / tag_variable[state]\n",
    "                                else:\n",
    "                                    b = 0\n",
    "                    if sentence[t] in [\"'m\", \"'s\", \"'re\", \"are\", \"is\", \"am\", \"was\", \"were\", \"have\", \"do\", \"did\",\"had\"]:\n",
    "                        if len(sentence) - 1 > t:\n",
    "                            if ((sentence[t] == \"have\" and sentence[t + 1] == \"to\") or (\n",
    "                                    sentence[t] in [\"do\", \"did\",\"does\"] and sentence[t + 1] == \"n't\")) and state == 'AUX':\n",
    "                                b = 1\n",
    "                            elif len(sentence[t + 1]) > 3:\n",
    "                                if sentence[t + 1][-3:] == \"ing\" and state == 'AUX':\n",
    "                                    b = 1\n",
    "                    if len(sentence)-1>t and sentence[t] == 'to': #handling \"to\" =) by looking into the future\n",
    "                        max_occur = 0\n",
    "                        max_tag = None\n",
    "                        for tup,val in emission.items():\n",
    "                            if sentence[t+1] in tup and (val > max_occur):\n",
    "                                max_occur = val\n",
    "                                max_tag = tup[0]\n",
    "                        if max_tag == 'VERB' and state == 'PART':\n",
    "                            b=1\n",
    "                        if max_tag != 'VERB' and state == 'PART':\n",
    "                            b=0\n",
    "\n",
    "                    links_value.append(viterbi_matrix[previous_step_state][t - 1] * a * b)\n",
    "                    backtrack_values.append(viterbi_matrix[previous_step_state][t - 1] * a)\n",
    "                viterbi_matrix[state].append(max(links_value))\n",
    "                backpointer[state].append(state_graph[np.argmax(backtrack_values)])\n",
    "        # termination step\n",
    "        links_value = []\n",
    "        backtrack_values = []\n",
    "\n",
    "        for state in state_graph:\n",
    "            # tag_variable = tag_count\n",
    "            if tuple([state,'END']) in transition_variable:  # not new transition between tags\n",
    "                a = transition_variable[tuple([state,'END'])] / tag_variable[state]\n",
    "            else:\n",
    "                a = 0\n",
    "            links_value.append(viterbi_matrix[state][len(sentence) - 1] * a)\n",
    "            backtrack_values.append(viterbi_matrix[state][len(sentence) - 1] * a)\n",
    "        viterbi_matrix['END'] = []\n",
    "        viterbi_matrix['END'].append(max(links_value))\n",
    "        backpointer['END'] = []\n",
    "        backpointer['END'].append(state_graph[np.argmax(backtrack_values)])\n",
    "        path = backpointer[backpointer['END'][0]][1:]\n",
    "        path.append(backpointer['END'][0])\n",
    "        for word, l, tag in zip(sentence, lang, path):\n",
    "            print(word + '\\t' + l + '\\t' + tag)\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "def cmpFiles(experiment_result_file, true_result_file):#comapre 2 files line by line\n",
    "    experiment_result = []\n",
    "    true_result = []\n",
    "    result = True\n",
    "    negative_results = 0\n",
    "    total_number_results = 0\n",
    "    for line in open(experiment_result_file, 'r'):\n",
    "        if line.split():\n",
    "            line_list = []\n",
    "            for ids in line.split():\n",
    "                line_list.append(ids)\n",
    "            experiment_result.append(line_list)\n",
    "            total_number_results += 1\n",
    "    set_of_words =set()\n",
    "    for line in open(true_result_file, 'r',encoding ='UTF-8'):\n",
    "        if line.split():\n",
    "            line_list = []\n",
    "            for ids in line.split():\n",
    "                line_list.append(ids)\n",
    "            set_of_words.add(line_list[0])\n",
    "            true_result.append(line_list)\n",
    "    line_counter=1\n",
    "    for word in set_of_words:\n",
    "        for trueLine, expLine in zip(true_result, experiment_result):\n",
    "            if trueLine != expLine and expLine[0] == word:\n",
    "                negative_results += 1\n",
    "                print('True = ',trueLine,'Exper = ',expLine, 'Line = ',line_counter)\n",
    "            line_counter += 1\n",
    "    return round((total_number_results - negative_results) / total_number_results, 4)\n",
    "\n",
    "\n",
    "\n",
    "def main(trainFilePath,testFilePath):\n",
    "    return trainFilePath,testFilePath\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) > 3:\n",
    "        trainFilePath,testFilePath = main(sys.argv[1],sys.argv[2])\n",
    "\n",
    "emission_count,spa_emiss,eng_emiss,tag_count_dictionary,spa_tag,eng_tag, transition_count, word_counting, sentence_counter,tok_len = counting_tri_bi_uni_grams(trainFilePath)\n",
    "state_graph = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT',\n",
    "               'SCONJ', 'SYM', 'VERB', 'X', 'UNK']\n",
    "lambda_param = deleted_interpolation(transition_count,tag_count_dictionary, word_counting)\n",
    "\n",
    "viterbi(testFilePath, state_graph, tag_count_dictionary, transition_count,emission_count, word_counting, \\\n",
    "        lambda_param, spa_emiss,eng_emiss,spa_tag,eng_tag,tok_len)\n",
    "'''\n",
    "orig_stdout = sys.stdout\n",
    "fout = open('wrongpredictions.txt', 'w')\n",
    "sys.stdout = fout\n",
    "print('Accuracy ',cmpFiles('submission.txt','dataset/dev.conll'))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
